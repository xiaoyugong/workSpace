package competition;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map.Entry;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import competition.XmlParser.XmlInputFormat;

public class MeterCompute extends Configured implements Tool {
	
	/**
	 * 初始化HBase表参数
	 * BaseInfoTableName：存储xml文件基础信息的表名
	 * BaseInfoColumnFamily：BaseInfoTableName表的列族
	 * ComputeResultTableName：存储xml电流变化相关信息的表名
	 * ComputeResultColumnFamily：ComputeResultTableName表的列族
	 */
	
	//############################# 代码部分1-1：请设置HBase表参数  #############################
	static HBaseOps hbaseops;
	static TableName BaseInfoTableName = TableName.valueOf("BaseInfo");
	static String[] BaseInfoColumnFamily = {"Info","Function"};
	static TableName ComputeResultTableName = TableName.valueOf("ComputeResult");
	static String[] ComputeResultColumnFamily = {"ComputeInfo"};
	//################################################################################

	public static void main(String[] args) throws Exception {
	//############################# 代码部分1-2：请创建HBase表  #############################
//		hbaseops = Tools.getHBase();
		hbaseops = HBaseOps.connectHbase();
		hbaseops.createTable(BaseInfoTableName, BaseInfoColumnFamily);
		hbaseops.createTable(ComputeResultTableName, ComputeResultColumnFamily);
	//################################################################################
		while (true) {
			ToolRunner.run(new MeterCompute(), args);
			//核心程序循环间隔时间
			Thread.sleep(4000);
		}
	}
	/**
	 * run方法中定义了Job的配置，然后提交作业；同时处理数据文件
	 */
	public int run(String[] args) throws Exception {
		
		String InDirPath = args[0];
		String OutDirPath = args[1];
		Configuration conf = new Configuration();

		FileSystem fs = FileSystem.get(URI.create(InDirPath), conf);

		// 设置xml文件解析的开始和结束位置
		conf.set("xmlinput.start", "<root>");
		conf.set("xmlinput.end", "</root>");

		@SuppressWarnings("deprecation")
		
		//############################# 代码部分2：请设置Job参数  ##############################
		// 初始化job
		Job job = new Job(conf);
		// 设置JarByClass
		job.setJarByClass(MeterCompute.class);
		// 设置Map阶段输出的Key和Value的类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		// 指定Map类
		job.setMapperClass(CustomerMap.class);
		// 指定Reduce类
		job.setReducerClass(CustomerReduce.class);
		// 设置input格式的类
		job.setInputFormatClass(XmlInputFormat.class);
		// 设置output格式的类
		job.setOutputFormatClass(TextOutputFormat.class);
		//###############################################################################
		
		//从HDFS指定的目录下读取所有文件,并保存该批次的文件信息
		FileInputFormat.addInputPath(job, new Path(InDirPath + "/input/xml"));
		FileStatus[] fileStatus = fs.listStatus(new Path(InDirPath + "/input/xml"));
		
		// 设置输出目录
		FileOutputFormat.setOutputPath(job, new Path(OutDirPath + "/output"));	
		// 输出目录若已存在，则删除
		Path outPath = new Path(OutDirPath + "/output");
		if (fs.exists(outPath)) {
			fs.delete(outPath, true);
		}
		
		// 提交job
		job.waitForCompletion(true);
		
		// 判断保存已处理文件的tmp文件夹是否存在，不存在则创建
		if(!(fs.exists(new Path(InDirPath + "/tmp/xml")))){
			fs.mkdirs(new Path(InDirPath + "/tmp/xml"));
		}
		// 清空hdfs://....../input/xml下的所有XML文件，移动到hdfs://....../tmp/xml
		for (int i = 0; i < fileStatus.length; i++) {
			FileStatus fileStatu = fileStatus[i];
			String filename = fileStatu.getPath().toString().substring(fileStatu.getPath().toString().lastIndexOf("/"), fileStatu.getPath().toString().length());
			String newFilename = InDirPath + "/tmp/xml" + filename;
			fs.rename(fileStatu.getPath(), new Path(newFilename));
		}
		
		System.out.println("程序已执行第 " + (++Tools.runCounter) + " 次");
		System.out.println("本次处理 " + Tools.fileCounter + " 个文件");
		Tools.fileCounter = 0;
		
		return 0;
	}
	
	/**
	 * 
	 *  CustomerMap中定义了map程序，在map中调用Tools类中的initXml方法直接解析读取到的文件内容
	 *  解析结果以HashMap保存，并提供了3个API获取想要的字段数据
	 *	基于获取的字段数据需要完成1.将全部基础信息存入BaseInfo表 2.将电流差值计算所需数据传入context
	 */
	
	public static class CustomerMap extends Mapper<LongWritable, Text, Text, Text> {
		//初始化一个Tools类
		Tools t = new Tools();
		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {
			Tools.fileCounter += 1;
			String document = value.toString();
			
			//############################# 代码部分3：请完善map代码  ##############################
			//调用Tools类的initXml()方法，将得到的xml进行解析
			t.initXml(document);
			//获取以HashMap保存公共标签信息的对象common
			HashMap<String,String> common = t.getPublic();
			//从common中获取采集器id (gatewayid)和时间(sequence)
			String gatewayId = common.get("gateway_id");
			String sequence =  common.get("sequence");
			//调用Tools类的getContent()方法获取保存了meter和function信息的HashMap，此HashMap的key为meterid,value为hashMap
			HashMap<String,HashMap<String,String>> contentMap = t.getContent();

			//遍历得到的HashMap的key和value
			Iterator<Entry<String, HashMap<String,String>>> contentIter = contentMap.entrySet().iterator();  
			while (contentIter.hasNext()) {  
				Entry<String, HashMap<String,String>> entry = (Entry<String, HashMap<String,String>>) contentIter.next();  
				
				//获取meterID
				String meterId = (String)entry.getKey();
				
				//将时间与同一个采集器中各电表的电流值传到reduce做相应计算
				String electricity = entry.getValue().get("text_1").toString();  
				context.write(new Text(gatewayId + "_" + meterId), new Text(sequence + ">" + electricity));
				
				//遍历common的key和value，将公共标签的信息写入到hbase表保存baseinfo信息的列族
				Iterator<Entry<String, String>> commonIter = common.entrySet().iterator(); 
				while(commonIter.hasNext()){
					Entry<String, String> commonEntry = (Entry<String, String>) commonIter.next();  
					String commonKey = commonEntry.getKey().toString();
					String commonValue = commonEntry.getValue().toString();
					try {
						hbaseops.put("BaseInfo", sequence + "_" + gatewayId + "_" + meterId, "Info", commonKey, commonValue);
					} catch (Exception e) {
						e.printStackTrace();
					}
				}
				//初始化一个保存function信息的HashMap对象function
				HashMap<String,String> function = contentMap.get(meterId);

				//遍历function的key和value，将function的信息写入到hbase表保存function信息的列族
				Iterator<Entry<String, String>> funcIter = function.entrySet().iterator(); 
				while(funcIter.hasNext()){
					Entry<String, String> funcentry = (Entry<String, String>) funcIter.next();  
					String funcKey = funcentry.getKey().toString();
					String funcValue = funcentry.getValue().toString();
					try {
						hbaseops.put("BaseInfo", sequence + "_" + gatewayId + "_" + meterId, "Function", funcKey, funcValue);
					} catch (Exception e) {
						e.printStackTrace();
					}
				}

			}
		}
		//################################################################################
	}
	/**
	 * 
	 * CustomerReduce中定义了reduce程序
	 * 在此需要把从context中获取的数据进行算法设计，获得目标值
	 * 需要注意的是，应该总是保留最后一个文件的数据，并在下次reduce计算时重新添加
	 */
	public static class CustomerReduce extends Reducer<Text, Text, Text, Text> {
		public void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {			
			// valueList保存value的集合
			List<String> valueList = new ArrayList<String>();			
			for (Text value : values) {
				valueList.add(value.toString());
			}
			//KVmap用于保留每个Key及对应的values中的最后一个value
			Map<String, String> KVmap = Tools.KVmap;
			// 排除第一次Reduce时KVmap中没有数据
			if(KVmap.containsKey(key.toString())){
				valueList.add(KVmap.get(key.toString()));
			}
			// 对value集合按照时间进行排序
			Collections.sort(valueList);		
			// 将该key中最后一个value进行中间存储，便于下轮Job继续作差值
			KVmap.put(key.toString(), valueList.get(valueList.size()-1));
			
			//############################# 代码部分4：请完善reduce代码  ##############################
			// 第一个function的值为电流值
			int electricity;
			//保存map阶段传过来的时间
			String sequence = "";
			// 以10秒为单位,计算同一楼栋同一采集器电流变化值
			for (int i = 0; i < valueList.size() - 1; i++) {
				sequence = valueList.get(i).split(">")[0];
				electricity = Integer.parseInt(valueList.get(i).split(">")[1]);
				String startTime = sequence;
				String endTime = valueList.get(i + 1).split(">")[0];
				// newValue电流变化的差值
				int newValue = Integer.parseInt(valueList.get(i + 1).split(">")[1]) - electricity;
				
				// 将得到的数据存入hbase，RowKey为gateway_id+meterid+startTime
				try {
					hbaseops.put(ComputeResultTableName.toString(), startTime + "_" + key.toString(), "ComputeInfo", "startTime", startTime);
					hbaseops.put(ComputeResultTableName.toString(), startTime + "_" + key.toString(), "ComputeInfo", "endTime", endTime);
					hbaseops.put(ComputeResultTableName.toString(), startTime + "_" + key.toString(), "ComputeInfo", "change", String.valueOf(newValue));
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
			//################################################################################
		}
	}
}
